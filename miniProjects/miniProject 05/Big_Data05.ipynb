{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Big_Data05.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Name - Ambuj Mishra\n",
        "# Student ID - 202116003\n",
        "\n",
        "##### This is a colab PDF containing both question codes and the description required in the assignment."
      ],
      "metadata": {
        "id": "A9maItP8P7Zl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Spark Initialization"
      ],
      "metadata": {
        "id": "zmA0CYUzImV-"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DKmUHlrOlH5W"
      },
      "outputs": [],
      "source": [
        "# innstall java\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "\n",
        "# install spark (change the version number if needed)\n",
        "!wget -q https://archive.apache.org/dist/spark/spark-3.0.0/spark-3.0.0-bin-hadoop3.2.tgz\n",
        "\n",
        "# unzip the spark file to the current folder\n",
        "!tar xf spark-3.0.0-bin-hadoop3.2.tgz\n",
        "\n",
        "# set your spark folder to your system path environment. \n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.0.0-bin-hadoop3.2\"\n",
        "\n",
        "\n",
        "# install findspark using pip\n",
        "!pip install -q findspark"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import findspark\n",
        "\n",
        "findspark.init()"
      ],
      "metadata": {
        "id": "v-Qt5n0toW7U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder\\\n",
        "        .master(\"local\")\\\n",
        "        .appName(\"Colab\")\\\n",
        "        .config('spark.ui.port', '4050')\\\n",
        "        .getOrCreate()"
      ],
      "metadata": {
        "id": "D04uVzcWolf8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "type(spark)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ylvH_dliouUA",
        "outputId": "99791213-0015-4ef0-8046-ed66d1d19559"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "pyspark.sql.session.SparkSession"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sc=spark.sparkContext"
      ],
      "metadata": {
        "id": "9zhI9zaFo-pa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Question-1"
      ],
      "metadata": {
        "id": "UAxNCuaBpCri"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!head /content/2015-summary.csv"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3LmH81PcqVQK",
        "outputId": "59fed12e-8c6a-47a6-8746-9609497ca913"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DEST_COUNTRY_NAME,ORIGIN_COUNTRY_NAME,count\n",
            "United States,Romania,15\n",
            "United States,Croatia,1\n",
            "United States,Ireland,344\n",
            "Egypt,United States,15\n",
            "United States,India,62\n",
            "United States,Singapore,1\n",
            "United States,Grenada,62\n",
            "Costa Rica,United States,588\n",
            "Senegal,United States,40\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "flightData2015 = spark.read.option(\"inferSchema\", \"true\").option(\"header\", \"true\").csv(\"/content/2015-summary.csv\")"
      ],
      "metadata": {
        "id": "vGfkqF-OpGJ6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "flightData2015.take(3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C7mUHksCpV2Z",
        "outputId": "d277f7a8-962e-44aa-961d-7fa2f469e911"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Romania', count=15),\n",
              " Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Croatia', count=1),\n",
              " Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Ireland', count=344)]"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "flightData2015.sort(\"count\").explain()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kbLgrIMBqh7s",
        "outputId": "7af2286e-9856-46b3-eeb7-262e86602327"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "== Physical Plan ==\n",
            "*(1) Sort [count#18 ASC NULLS FIRST], true, 0\n",
            "+- Exchange rangepartitioning(count#18 ASC NULLS FIRST, 200), true, [id=#32]\n",
            "   +- FileScan csv [DEST_COUNTRY_NAME#16,ORIGIN_COUNTRY_NAME#17,count#18] Batched: false, DataFilters: [], Format: CSV, Location: InMemoryFileIndex[file:/content/2015-summary.csv], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<DEST_COUNTRY_NAME:string,ORIGIN_COUNTRY_NAME:string,count:int>\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "spark.conf.set(\"spark.sql.shuffle.partitions\", \"5\")"
      ],
      "metadata": {
        "id": "5712wEjvqm_K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "flightData2015.sort(\"count\").take(2)"
      ],
      "metadata": {
        "id": "dza-D5yRqth-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dc273e18-e274-4859-d4b1-1edc33621c99"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Singapore', count=1),\n",
              " Row(DEST_COUNTRY_NAME='Moldova', ORIGIN_COUNTRY_NAME='United States', count=1)]"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## DataFrames and SQL"
      ],
      "metadata": {
        "id": "JDtCqEv3KxaY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "flightData2015.createOrReplaceTempView(\"flight_data_2015\")"
      ],
      "metadata": {
        "id": "j5kU-C4rKjaU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sqlWay = spark.sql(\"\"\"SELECT DEST_COUNTRY_NAME, count(1) FROM flight_data_2015 GROUP BY DEST_COUNTRY_NAME\"\"\")\n",
        "dataFrameWay = flightData2015.groupBy(\"DEST_COUNTRY_NAME\").count()\n",
        "\n",
        "sqlWay.explain()\n",
        "dataFrameWay.explain()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cEikiHlNKto6",
        "outputId": "647d1b2a-b3b6-4291-b918-c342052aa1ce"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "== Physical Plan ==\n",
            "*(2) HashAggregate(keys=[DEST_COUNTRY_NAME#16], functions=[count(1)])\n",
            "+- Exchange hashpartitioning(DEST_COUNTRY_NAME#16, 5), true, [id=#61]\n",
            "   +- *(1) HashAggregate(keys=[DEST_COUNTRY_NAME#16], functions=[partial_count(1)])\n",
            "      +- FileScan csv [DEST_COUNTRY_NAME#16] Batched: false, DataFilters: [], Format: CSV, Location: InMemoryFileIndex[file:/content/2015-summary.csv], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<DEST_COUNTRY_NAME:string>\n",
            "\n",
            "\n",
            "== Physical Plan ==\n",
            "*(2) HashAggregate(keys=[DEST_COUNTRY_NAME#16], functions=[count(1)])\n",
            "+- Exchange hashpartitioning(DEST_COUNTRY_NAME#16, 5), true, [id=#80]\n",
            "   +- *(1) HashAggregate(keys=[DEST_COUNTRY_NAME#16], functions=[partial_count(1)])\n",
            "      +- FileScan csv [DEST_COUNTRY_NAME#16] Batched: false, DataFilters: [], Format: CSV, Location: InMemoryFileIndex[file:/content/2015-summary.csv], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<DEST_COUNTRY_NAME:string>\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "spark.sql(\"SELECT max(count) from flight_data_2015\").take(1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mn-ODtOmLEIi",
        "outputId": "d7e00630-74b8-4a45-8edd-c5c5ac1f64cd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Row(max(count)=370002)]"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import max\n",
        "\n",
        "flightData2015.select(max(\"count\")).take(1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QOXsJoHFLPZU",
        "outputId": "4535a548-8ac0-4961-93c2-f1da1f517b5f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Row(max(count)=370002)]"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "maxSql = spark.sql(\"\"\"SELECT DEST_COUNTRY_NAME, sum(count) as destination_total FROM flight_data_2015 GROUP BY DEST_COUNTRY_NAME ORDER BY sum(count) DESC LIMIT 5\"\"\")\n",
        "maxSql.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r4HdmZ0SLTsN",
        "outputId": "f151a6ce-1d86-43d7-c5f3-42ae1addb018"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------------+-----------------+\n",
            "|DEST_COUNTRY_NAME|destination_total|\n",
            "+-----------------+-----------------+\n",
            "|    United States|           411352|\n",
            "|           Canada|             8399|\n",
            "|           Mexico|             7140|\n",
            "|   United Kingdom|             2025|\n",
            "|            Japan|             1548|\n",
            "+-----------------+-----------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import desc\n",
        "flightData2015\\\n",
        ".groupBy(\"DEST_COUNTRY_NAME\")\\\n",
        ".sum(\"count\")\\\n",
        ".withColumnRenamed(\"sum(count)\", \"destination_total\")\\\n",
        ".sort(desc(\"destination_total\"))\\\n",
        ".limit(5)\\\n",
        ".show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SShi5uqqLe6y",
        "outputId": "3d04ddd5-7f7c-4eb9-b210-09babe92a049"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------------+-----------------+\n",
            "|DEST_COUNTRY_NAME|destination_total|\n",
            "+-----------------+-----------------+\n",
            "|    United States|           411352|\n",
            "|           Canada|             8399|\n",
            "|           Mexico|             7140|\n",
            "|   United Kingdom|             2025|\n",
            "|            Japan|             1548|\n",
            "+-----------------+-----------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "flightData2015\\\n",
        ".groupBy(\"DEST_COUNTRY_NAME\")\\\n",
        ".sum(\"count\")\\\n",
        ".withColumnRenamed(\"sum(count)\", \"destination_total\")\\\n",
        ".sort(desc(\"destination_total\"))\\\n",
        ".limit(5)\\\n",
        ".explain()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eq8Mzg8aLpXZ",
        "outputId": "556c0e57-ca8b-4d21-d418-db5c2444e812"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "== Physical Plan ==\n",
            "TakeOrderedAndProject(limit=5, orderBy=[destination_total#104L DESC NULLS LAST], output=[DEST_COUNTRY_NAME#16,destination_total#104L])\n",
            "+- *(2) HashAggregate(keys=[DEST_COUNTRY_NAME#16], functions=[sum(cast(count#18 as bigint))])\n",
            "   +- Exchange hashpartitioning(DEST_COUNTRY_NAME#16, 5), true, [id=#227]\n",
            "      +- *(1) HashAggregate(keys=[DEST_COUNTRY_NAME#16], functions=[partial_sum(cast(count#18 as bigint))])\n",
            "         +- FileScan csv [DEST_COUNTRY_NAME#16,count#18] Batched: false, DataFilters: [], Format: CSV, Location: InMemoryFileIndex[file:/content/2015-summary.csv], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<DEST_COUNTRY_NAME:string,count:int>\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Physical Plan-\n",
        "\n",
        "Physical Plan is an internal enhancement or optimization for Spark. It is generated after creation of the Optimized Logical Plan. Physical Plan is limited to Spark operation and for this, it will do an evaluation of multiple physical plans and finalize the suitable optimal physical plan. And ultimately, the finest Physical Plan runs. Once the finest Physical Plan is selected, executable code (DAG of RDDs) for the query is created which needs to be executed in a distributed manner on the cluster."
      ],
      "metadata": {
        "id": "Xa1v4PszOwMh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Question-2"
      ],
      "metadata": {
        "id": "qDhRYn-1L6PC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Basic Query Example"
      ],
      "metadata": {
        "id": "moJMLZeLUZ2d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import Spark SQL\n",
        "from pyspark.sql import HiveContext, Row\n",
        "# Or if you can't include the hive requirements\n",
        "from pyspark.sql import SQLContext, Row"
      ],
      "metadata": {
        "id": "EJPE9siVL-L9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "hiveCtx = HiveContext(sc)"
      ],
      "metadata": {
        "id": "2yuyVhuHNMp-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input = hiveCtx.read.json(\"/content/testweet.json\")\n",
        "input.show()\n",
        "\n",
        "# Register the input schema RDD\n",
        "input.registerTempTable(\"tweets\")\n",
        "# Select tweets based on the retweetCount\n",
        "topTweets = hiveCtx.sql(\"\"\"SELECT text, retweetCount FROM tweets ORDER BY retweetCount LIMIT 10\"\"\")\n",
        "topTweets.collect()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GzHvAIS1NRUq",
        "outputId": "497c39d9-193a-4464-ca43-f894e1c38a89"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------------+--------------------+--------------------+---------------+------------------+-----------------+---------------+-----------+-------------------+-----------+-------------+------------+--------------------+--------------------+-----------+--------------------+-------------------+\n",
            "|contributorsIDs|           createdAt|currentUserRetweetId|hashtagEntities|                id|inReplyToStatusId|inReplyToUserId|isFavorited|isPossiblySensitive|isTruncated|mediaEntities|retweetCount|              source|                text|urlEntities|                user|userMentionEntities|\n",
            "+---------------+--------------------+--------------------+---------------+------------------+-----------------+---------------+-----------+-------------------+-----------+-------------+------------+--------------------+--------------------+-----------+--------------------+-------------------+\n",
            "|             []|Nov 4, 2014 4:56:...|                  -1|             []|529799371026485248|               -1|             -1|      false|              false|      false|           []|           0|<a href=\"http://t...|Adventures With C...|         []|[Aug 5, 2011 9:42...|                 []|\n",
            "+---------------+--------------------+--------------------+---------------+------------------+-----------------+---------------+-----------+-------------------+-----------+-------------+------------+--------------------+--------------------+-----------+--------------------+-------------------+\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Row(text='Adventures With Coffee, Code, and Writing.', retweetCount=0)]"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Spark SQL UDF"
      ],
      "metadata": {
        "id": "CRqM0VywUhsD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.types import IntegerType\n",
        "\n",
        "# Make a UDF to tell us how long some text is\n",
        "hiveCtx.registerFunction(\"strLenPython\", lambda x: len(x), IntegerType())\n",
        "lengthSchemaRDD = hiveCtx.sql(\"SELECT strLenPython('text') FROM tweets LIMIT 10\")\n",
        "lengthSchemaRDD.collect()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OEZOnUReUlyL",
        "outputId": "3bacc55d-20cc-43f4-df66-0ce668577d77"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Row(strLenPython(text)=4)]"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Example 9-40"
      ],
      "metadata": {
        "id": "bwraz4PcVmzM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "multipleSumRDD = hiveCtx.sql(\"SELECT SUM(user.favouritesCount), SUM(retweetCount), user.id FROM tweets GROUP BY user.id\")\n",
        "multipleSumRDD.collect()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bDU4lv_jVppv",
        "outputId": "31f3e727-c737-42b1-d4bd-234352d3794c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Row(sum(user.favouritesCount AS `favouritesCount`)=1095, sum(retweetCount)=0, id=15594928)]"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Ideas about Sentiment Analysis in Spark\n",
        "\n",
        "Sentiment Analysis in Spark is doable and feasible because of Spark's feature of live data handling and even if we want to perform offline stored tweet's sentiment analysis, still Spark provides  proper platform for that.\n",
        "\n",
        "## Steps for Sentiment Analysis\n",
        "\n",
        "Following steps are required if we want to perform sentiment analysis in Spark-\n",
        "\n",
        "1. Data Preprocessing-\n",
        "\n",
        "We need to clean our tweets and remove unnecessary information. For that we'll first tokenize our dataset and create tokens out of it. On the basis of that, we'll remove stopwords and not required tags and symbols.\n",
        "\n",
        "2. Stemming or Lemmatization-\n",
        "\n",
        "We will convert our tokens into their stem form or lemma form to equate the same stem/lemma words.\n",
        "\n",
        "3. Vectorize dataset-\n",
        "\n",
        "On the basis on the filtered data, we'll generate the vectors for every tweet.\n",
        "\n",
        "4. Sentiment Classification-\n",
        "\n",
        "On the basis of our requirement, we'll generate classes of dataset and based on the vectorized data, we'll train the model using some classifcation algorithm. This model will further help in generating sentiment results."
      ],
      "metadata": {
        "id": "UwkvjomeJq9j"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Question-3"
      ],
      "metadata": {
        "id": "6ipFXxxyWioD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Example : Spam Classification"
      ],
      "metadata": {
        "id": "Duf1Uyt5I8j2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.mllib.regression import LabeledPoint\n",
        "from pyspark.mllib.feature import HashingTF\n",
        "from pyspark.mllib.classification import LogisticRegressionWithSGD\n",
        "\n",
        "spam = sc.textFile(\"spam.txt\")\n",
        "normal = sc.textFile(\"normal.txt\")\n",
        "\n",
        "# Create a HashingTF instance to map email text to vectors of 150 features.\n",
        "tf = HashingTF(numFeatures = 150)\n",
        "\n",
        "# Each email is split into words, and each word is mapped to one feature.\n",
        "spamFeatures = spam.map(lambda email: tf.transform(email.split(\" \")))\n",
        "normalFeatures = normal.map(lambda email: tf.transform(email.split(\" \")))\n",
        "\n",
        "# Create LabeledPoint datasets for positive (spam) and negative (normal) examples.\n",
        "positiveExamples = spamFeatures.map(lambda features: LabeledPoint(1, features))\n",
        "negativeExamples = normalFeatures.map(lambda features: LabeledPoint(0, features))\n",
        "trainingData = positiveExamples.union(negativeExamples)\n",
        "trainingData.cache() # Cache since Logistic Regression is an iterative algorithm.\n",
        "\n",
        "# Run Logistic Regression using the SGD algorithm.\n",
        "model = LogisticRegressionWithSGD.train(trainingData)\n",
        "\n",
        "# Test on a positive example (spam) and a negative one (normal). We first apply\n",
        "# the same HashingTF feature transformation to get vectors, then apply the model.\n",
        "posTest = tf.transform(\"O M G GET cheap stuff by sending money to ...\".split(\" \"))\n",
        "negTest = tf.transform(\"Hi Dad, I started studying Spark the other ...\".split(\" \"))\n",
        "\n",
        "print(\"Prediction for positive test example: {}\".format(model.predict(posTest)))\n",
        "print(\"Prediction for negative test example: {}\".format(model.predict(negTest)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z99R2Ec4WkZv",
        "outputId": "d05b02d7-cbcc-4ea1-8158-95f5f5c70eb3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prediction for positive test example: 1\n",
            "Prediction for negative test example: 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Hashing TF\n",
        "\n",
        "Using hashing TF, we generate hash code of any object which is irreversible in nature. Since, hashing is not reversible, you cannot restore original input from a hash vector. On the other hand, count vector with model (index) can be used to restore unordered input. As a consequence models created using hashed input can be much harder to interpret and monitor.\n",
        "\n",
        "## Logistic Regression\n",
        "\n",
        "Logistic Regression in Spark can be implemented using *org.apache.spark.ml.classification.LogisticRegression* module.\n",
        "\n",
        "In case of binary classification using logisticRegression, we assign first class as '0' and other one as '1'. We further try to calculate the probility of belongingness in class 1(y==1) given a sample 'x'.\n",
        "\n",
        "P(y==1/x) = 1/ (1+e^-x)\n",
        "\n",
        "Based on the probability, we  generate the final discriminant function.\n",
        "\n",
        "G(x) = G1(x) - G0(x)\n",
        "\n",
        "Now, we can say if it is greater than 0, then the given sample will belong to class 1 otherwise in class 0.\n",
        "\n",
        "In case of multiclass classification, we use one vs rest or pairwise approach to discriminate."
      ],
      "metadata": {
        "id": "S_Fsuve4MDW3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# THANK YOU!"
      ],
      "metadata": {
        "id": "eTFer3X5JDkD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4HO1pLNZQqLu",
        "outputId": "1deccc88-4759-44e4-ce7c-0ce20efd2428"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget -nc https://raw.githubusercontent.com/brpy/colab-pdf/master/colab_pdf.py\n",
        "from colab_pdf import colab_pdf\n",
        "colab_pdf('Big_Data05.ipynb')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 321
        },
        "id": "9dTumY97QqFn",
        "outputId": "f352cd23-8b2f-4273-8661-b86b5395d034"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "File ‘colab_pdf.py’ already there; not retrieving.\n",
            "\n",
            "\n",
            "WARNING: apt does not have a stable CLI interface. Use with caution in scripts.\n",
            "\n",
            "\n",
            "WARNING: apt does not have a stable CLI interface. Use with caution in scripts.\n",
            "\n",
            "[NbConvertApp] Converting notebook /content/drive/MyDrive/Colab Notebooks/Big_Data05.ipynb to pdf\n",
            "[NbConvertApp] Writing 54491 bytes to ./notebook.tex\n",
            "[NbConvertApp] Building PDF\n",
            "[NbConvertApp] Running xelatex 3 times: ['xelatex', './notebook.tex', '-quiet']\n",
            "[NbConvertApp] Running bibtex 1 time: ['bibtex', './notebook']\n",
            "[NbConvertApp] WARNING | bibtex had problems, most likely because there were no citations\n",
            "[NbConvertApp] PDF successfully created\n",
            "[NbConvertApp] Writing 73976 bytes to /content/drive/My Drive/Big_Data05.pdf\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_a6e17ada-14b2-4d3e-999e-751cef124dbd\", \"Big_Data05.pdf\", 73976)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'File ready to be Downloaded and Saved to Drive'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "YbeFeKbZQ5DX"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}